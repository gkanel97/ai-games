{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent(game_instance, agent, opponent, num_games=1000):\n",
    "    agent.in_training = False\n",
    "    scores = {'X': 0, 'O': 0, 'tie': 0}\n",
    "    # pbar = tqdm(total=num_games)\n",
    "    for i in range(num_games):\n",
    "        while not game_instance.is_gameover():\n",
    "            if game_instance.player_X_turns:\n",
    "                agent.take_turn()\n",
    "            else:\n",
    "                opponent.take_turn()\n",
    "        if game_instance.X_wins:\n",
    "            scores['X'] += 1\n",
    "        elif game_instance.O_wins:\n",
    "            scores['O'] += 1\n",
    "        else:\n",
    "            scores['tie'] += 1\n",
    "        # pbar.update(1)\n",
    "        game_instance.play_again()\n",
    "    # pbar.close()\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tic-Tac-Toe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TicTacToeGame import TicTacToeGame\n",
    "from TicTacToeRandomSolver import TicTacToeRandomSolver\n",
    "from TicTacToeMinimaxSolver import TicTacToeMinimaxSolver\n",
    "from TicTacToeDefaultSolver import TicTacToeDefaultSolver\n",
    "from TicTacToeQLearningSolver import TicTacToeQLearningSolver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(agent, episodes):\n",
    "    agent.in_training = True\n",
    "    agent.train(episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [02:09<00:00,  1.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimax vs Default\n",
      "X wins: 95\n",
      "O wins: 0\n",
      "Ties: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [02:08<00:00,  1.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimax vs Default\n",
      "X wins: 93\n",
      "O wins: 0\n",
      "Ties: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [02:08<00:00,  1.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimax vs Default\n",
      "X wins: 91\n",
      "O wins: 0\n",
      "Ties: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [02:07<00:00,  1.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimax vs Default\n",
      "X wins: 94\n",
      "O wins: 0\n",
      "Ties: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:58<00:00,  1.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimax vs Default\n",
      "X wins: 87\n",
      "O wins: 0\n",
      "Ties: 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:48<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimax vs Default\n",
      "X wins: 93\n",
      "O wins: 0\n",
      "Ties: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:58<00:00,  1.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimax vs Default\n",
      "X wins: 89\n",
      "O wins: 0\n",
      "Ties: 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:47<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimax vs Default\n",
      "X wins: 90\n",
      "O wins: 0\n",
      "Ties: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:50<00:00,  1.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimax vs Default\n",
      "X wins: 84\n",
      "O wins: 0\n",
      "Ties: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:49<00:00,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimax vs Default\n",
      "X wins: 90\n",
      "O wins: 0\n",
      "Ties: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a game instance\n",
    "game_instance = TicTacToeGame()\n",
    "\n",
    "# Minimax agent plays against default opponent\n",
    "agent = TicTacToeMinimaxSolver(game_instance)\n",
    "opponent = TicTacToeDefaultSolver(game_instance)\n",
    "\n",
    "scores_arr = []\n",
    "for i in range(10):\n",
    "    scores = evaluate_agent(game_instance, agent, opponent, num_games=100)\n",
    "    scores_arr.append(scores)\n",
    "    print('Minimax vs Default')\n",
    "    print('X wins:', scores['X'])\n",
    "    print('O wins:', scores['O'])\n",
    "    print('Ties:', scores['tie'])\n",
    "\n",
    "with open('experiments/ttt-mini-vs-default.txt', 'w') as f:\n",
    "    json.dump(scores_arr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 35/1000 [00:00<00:02, 346.26it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:02<00:00, 492.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-learning vs Default - Untrained: X wins: 771, O wins: 188, tie: 41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Q-learning agent...: 100%|██████████| 5000/5000 [00:12<00:00, 408.97it/s]\n",
      "100%|██████████| 1000/1000 [00:01<00:00, 629.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-learning vs Random - Batch 0: X wins: 909, O wins: 59, tie: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Q-learning agent...: 100%|██████████| 5000/5000 [00:12<00:00, 412.20it/s]\n",
      "100%|██████████| 1000/1000 [00:01<00:00, 606.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-learning vs Random - Batch 1: X wins: 913, O wins: 22, tie: 65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Q-learning agent...: 100%|██████████| 5000/5000 [00:12<00:00, 414.50it/s]\n",
      "100%|██████████| 1000/1000 [00:01<00:00, 594.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-learning vs Random - Batch 2: X wins: 930, O wins: 23, tie: 47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Q-learning agent...: 100%|██████████| 5000/5000 [00:12<00:00, 398.02it/s]\n",
      "100%|██████████| 1000/1000 [00:02<00:00, 498.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-learning vs Random - Batch 3: X wins: 908, O wins: 0, tie: 92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Q-learning agent...: 100%|██████████| 5000/5000 [00:11<00:00, 436.34it/s]\n",
      "100%|██████████| 1000/1000 [00:01<00:00, 567.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-learning vs Random - Batch 4: X wins: 900, O wins: 22, tie: 78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Q-learning agent...: 100%|██████████| 5000/5000 [00:11<00:00, 436.61it/s]\n",
      "100%|██████████| 1000/1000 [00:01<00:00, 558.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-learning vs Random - Batch 5: X wins: 903, O wins: 0, tie: 97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Q-learning agent...: 100%|██████████| 5000/5000 [00:12<00:00, 407.94it/s]\n",
      "100%|██████████| 1000/1000 [00:01<00:00, 563.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-learning vs Random - Batch 6: X wins: 950, O wins: 0, tie: 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Q-learning agent...: 100%|██████████| 5000/5000 [00:12<00:00, 400.78it/s]\n",
      "100%|██████████| 1000/1000 [00:03<00:00, 301.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-learning vs Random - Batch 7: X wins: 955, O wins: 0, tie: 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Q-learning agent...: 100%|██████████| 5000/5000 [00:15<00:00, 325.25it/s]\n",
      "100%|██████████| 1000/1000 [00:01<00:00, 536.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-learning vs Random - Batch 8: X wins: 971, O wins: 0, tie: 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Q-learning agent...: 100%|██████████| 5000/5000 [00:13<00:00, 360.96it/s]\n",
      "100%|██████████| 1000/1000 [00:02<00:00, 428.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-learning vs Random - Batch 9: X wins: 979, O wins: 0, tie: 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a game instance\n",
    "game_instance = TicTacToeGame()\n",
    "\n",
    "# Minimax agent plays against default opponent\n",
    "agent = TicTacToeQLearningSolver(game_instance)\n",
    "opponent = TicTacToeDefaultSolver(game_instance)\n",
    "# opponent = TicTacToeRandomSolver(game_instance)\n",
    "\n",
    "untrained_scores = evaluate_agent(game_instance, agent, opponent)\n",
    "print('Q-learning vs Default - Untrained: X wins: {}, O wins: {}, tie: {}'.format(untrained_scores['X'], untrained_scores['O'], untrained_scores['tie']))\n",
    "q_learning_training = [untrained_scores]\n",
    "batch_train_episodes = 5000\n",
    "for i in range(10):\n",
    "    train_agent(agent, batch_train_episodes)\n",
    "    scores = evaluate_agent(game_instance, agent, opponent)\n",
    "    q_learning_training.append(scores)\n",
    "    print('Q-learning vs Default - Batch {}: X wins: {}, O wins: {}, tie: {}'.format(i, scores['X'], scores['O'], scores['tie']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('experiments/ttt-qlearning-vs-default.txt', 'w') as f:\n",
    "    json.dump(q_learning_training, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:16<00:00,  5.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimax vs Default\n",
      "X wins: 0\n",
      "O wins: 0\n",
      "Ties: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:16<00:00,  6.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimax vs Default\n",
      "X wins: 0\n",
      "O wins: 0\n",
      "Ties: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:15<00:00,  6.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimax vs Default\n",
      "X wins: 0\n",
      "O wins: 0\n",
      "Ties: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:15<00:00,  6.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimax vs Default\n",
      "X wins: 0\n",
      "O wins: 0\n",
      "Ties: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:22<00:00,  4.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimax vs Default\n",
      "X wins: 0\n",
      "O wins: 0\n",
      "Ties: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:19<00:00,  5.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimax vs Default\n",
      "X wins: 0\n",
      "O wins: 0\n",
      "Ties: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:19<00:00,  5.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimax vs Default\n",
      "X wins: 0\n",
      "O wins: 0\n",
      "Ties: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:18<00:00,  5.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimax vs Default\n",
      "X wins: 0\n",
      "O wins: 0\n",
      "Ties: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:18<00:00,  5.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimax vs Default\n",
      "X wins: 0\n",
      "O wins: 0\n",
      "Ties: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:16<00:00,  6.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimax vs Default\n",
      "X wins: 0\n",
      "O wins: 0\n",
      "Ties: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "opponent = TicTacToeMinimaxSolver(game_instance)\n",
    "scores_arr = []\n",
    "for i in range(10):\n",
    "    scores = evaluate_agent(game_instance, agent, opponent, num_games=100)\n",
    "    scores_arr.append(scores)\n",
    "    print('Minimax vs Default')\n",
    "    print('X wins:', scores['X'])\n",
    "    print('O wins:', scores['O'])\n",
    "    print('Ties:', scores['tie'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Q-learning agent...: 100%|██████████| 6000/6000 [00:17<00:00, 334.46it/s]\n",
      "Training Q-learning agent...: 100%|██████████| 100/100 [00:00<00:00, 358.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-learning vs Minimax - Batch 0: X wins: 0, O wins: 0, tie: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Q-learning agent...: 100%|██████████| 100/100 [00:00<00:00, 253.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-learning vs Minimax - Batch 1: X wins: 0, O wins: 100, tie: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Q-learning agent...: 100%|██████████| 100/100 [00:00<00:00, 324.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-learning vs Minimax - Batch 2: X wins: 0, O wins: 0, tie: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Q-learning agent...: 100%|██████████| 100/100 [00:00<00:00, 214.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-learning vs Minimax - Batch 3: X wins: 0, O wins: 100, tie: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Q-learning agent...: 100%|██████████| 100/100 [00:00<00:00, 136.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-learning vs Minimax - Batch 4: X wins: 0, O wins: 0, tie: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Q-learning agent...: 100%|██████████| 100/100 [00:00<00:00, 257.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-learning vs Minimax - Batch 5: X wins: 0, O wins: 0, tie: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Q-learning agent...: 100%|██████████| 100/100 [00:00<00:00, 276.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-learning vs Minimax - Batch 6: X wins: 0, O wins: 0, tie: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Q-learning agent...: 100%|██████████| 100/100 [00:00<00:00, 244.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-learning vs Minimax - Batch 7: X wins: 0, O wins: 100, tie: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Q-learning agent...: 100%|██████████| 100/100 [00:00<00:00, 208.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-learning vs Minimax - Batch 8: X wins: 0, O wins: 100, tie: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Q-learning agent...: 100%|██████████| 100/100 [00:00<00:00, 156.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-learning vs Minimax - Batch 9: X wins: 0, O wins: 100, tie: 0\n"
     ]
    }
   ],
   "source": [
    "# Create a game instance\n",
    "game_instance = TicTacToeGame()\n",
    "\n",
    "# Minimax agent plays against default opponent\n",
    "q_learning_agent = TicTacToeQLearningSolver(game_instance)\n",
    "minimax_agent = TicTacToeMinimaxSolver(game_instance)\n",
    "# opponent = TicTacToeRandomSolver(game_instance)\n",
    "\n",
    "q_learning_agent.train(episodes=6000)\n",
    "q_learning_training = [evaluate_agent(game_instance, q_learning_agent, minimax_agent, num_games=100)]\n",
    "batch_train_episodes = 100\n",
    "for i in range(10):\n",
    "    q_learning_agent.train(batch_train_episodes)\n",
    "    scores = evaluate_agent(game_instance, q_learning_agent, minimax_agent, num_games=100)\n",
    "    q_learning_training.append(scores)\n",
    "    print('Q-learning vs Minimax - Batch {}: X wins: {}, O wins: {}, tie: {}'.format(i, scores['X'], scores['O'], scores['tie']))\n",
    "\n",
    "with open('experiments/ttt-q-learning-vs-minimax.txt', 'w') as f:\n",
    "    json.dump(q_learning_training, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect 4 Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Connect4Game import Connect4Game\n",
    "from Connect4DefaultSolver import Connect4DefaultSolver\n",
    "from Connect4MinimaxSolver import Connect4MinimaxSolver\n",
    "from Connect4QLearningSolver import Connect4QLearningSolver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimax vs Default\n",
      "X wins: 9\n",
      "O wins: 991\n",
      "Ties: 0\n",
      "Minimax vs Default\n",
      "X wins: 1\n",
      "O wins: 998\n",
      "Ties: 1\n",
      "Minimax vs Default\n",
      "X wins: 2\n",
      "O wins: 998\n",
      "Ties: 0\n",
      "Minimax vs Default\n",
      "X wins: 1\n",
      "O wins: 999\n",
      "Ties: 0\n",
      "Minimax vs Default\n",
      "X wins: 0\n",
      "O wins: 1000\n",
      "Ties: 0\n"
     ]
    }
   ],
   "source": [
    "for depth in range(3, 8):\n",
    "    game_instance = Connect4Game()\n",
    "    minimax_agent = Connect4MinimaxSolver(game_instance, max_depth=depth, use_pruning=True)\n",
    "    default_opponent = Connect4DefaultSolver(game_instance)\n",
    "    scores = evaluate_agent(game_instance, minimax_agent, default_opponent, num_games=1000)\n",
    "    print('Minimax vs Default')\n",
    "    print('X wins:', scores['X'])\n",
    "    print('O wins:', scores['O'])\n",
    "    print('Ties:', scores['tie'])\n",
    "    with open('experiments/c4-minimax-vs-default-{}.txt'.format(depth), 'w') as f:\n",
    "        json.dump(scores, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('experiments/connect4-minimax-vs-default.txt', 'w') as f:\n",
    "    json.dump(scores, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimax vs Default\n",
      "X wins: 55\n",
      "O wins: 43\n",
      "Ties: 2\n",
      "Minimax vs Default\n",
      "X wins: 62\n",
      "O wins: 37\n",
      "Ties: 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n\u001b[1;32m     12\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time()\n\u001b[0;32m---> 13\u001b[0m     scores \u001b[38;5;241m=\u001b[39m evaluate_agent(game_instance, agent, opponent, num_games\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m     14\u001b[0m     end_time \u001b[38;5;241m=\u001b[39m time()\n\u001b[1;32m     15\u001b[0m     scores[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexecution_time\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m end_time \u001b[38;5;241m-\u001b[39m start_time\n",
      "Cell \u001b[0;32mIn[4], line 8\u001b[0m, in \u001b[0;36mevaluate_agent\u001b[0;34m(game_instance, agent, opponent, num_games)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m game_instance\u001b[38;5;241m.\u001b[39mis_gameover():\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m game_instance\u001b[38;5;241m.\u001b[39mplayer_X_turns:\n\u001b[0;32m----> 8\u001b[0m         agent\u001b[38;5;241m.\u001b[39mtake_turn()\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     10\u001b[0m         opponent\u001b[38;5;241m.\u001b[39mtake_turn()\n",
      "File \u001b[0;32m~/Documents/MSc Computer Science/CS7IS2 - Artificial Intelligence/Assignments/Assignment 3/code/Connect4MinimaxSolver.py:72\u001b[0m, in \u001b[0;36mConnect4MinimaxSolver.take_turn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtake_turn\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 72\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mminimax(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgame\u001b[38;5;241m.\u001b[39mmake_move(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchoice)\n",
      "File \u001b[0;32m~/Documents/MSc Computer Science/CS7IS2 - Artificial Intelligence/Assignments/Assignment 3/code/Connect4MinimaxSolver.py:46\u001b[0m, in \u001b[0;36mConnect4MinimaxSolver.minimax\u001b[0;34m(self, depth, isMaximizingPlayer, alpha, beta, use_pruning)\u001b[0m\n\u001b[1;32m     44\u001b[0m i \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(board[:, j] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m)[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     45\u001b[0m board[i, j] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m isMaximizingPlayer \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 46\u001b[0m scores\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mminimax(depth, \u001b[38;5;129;01mnot\u001b[39;00m isMaximizingPlayer, alpha, beta, use_pruning))\n\u001b[1;32m     47\u001b[0m moves\u001b[38;5;241m.\u001b[39mappend(j)\n\u001b[1;32m     48\u001b[0m board[i, j] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/Documents/MSc Computer Science/CS7IS2 - Artificial Intelligence/Assignments/Assignment 3/code/Connect4MinimaxSolver.py:46\u001b[0m, in \u001b[0;36mConnect4MinimaxSolver.minimax\u001b[0;34m(self, depth, isMaximizingPlayer, alpha, beta, use_pruning)\u001b[0m\n\u001b[1;32m     44\u001b[0m i \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(board[:, j] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m)[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     45\u001b[0m board[i, j] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m isMaximizingPlayer \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 46\u001b[0m scores\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mminimax(depth, \u001b[38;5;129;01mnot\u001b[39;00m isMaximizingPlayer, alpha, beta, use_pruning))\n\u001b[1;32m     47\u001b[0m moves\u001b[38;5;241m.\u001b[39mappend(j)\n\u001b[1;32m     48\u001b[0m board[i, j] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "    \u001b[0;31m[... skipping similar frames: Connect4MinimaxSolver.minimax at line 46 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m~/Documents/MSc Computer Science/CS7IS2 - Artificial Intelligence/Assignments/Assignment 3/code/Connect4MinimaxSolver.py:46\u001b[0m, in \u001b[0;36mConnect4MinimaxSolver.minimax\u001b[0;34m(self, depth, isMaximizingPlayer, alpha, beta, use_pruning)\u001b[0m\n\u001b[1;32m     44\u001b[0m i \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(board[:, j] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m)[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     45\u001b[0m board[i, j] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m isMaximizingPlayer \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 46\u001b[0m scores\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mminimax(depth, \u001b[38;5;129;01mnot\u001b[39;00m isMaximizingPlayer, alpha, beta, use_pruning))\n\u001b[1;32m     47\u001b[0m moves\u001b[38;5;241m.\u001b[39mappend(j)\n\u001b[1;32m     48\u001b[0m board[i, j] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/Documents/MSc Computer Science/CS7IS2 - Artificial Intelligence/Assignments/Assignment 3/code/Connect4MinimaxSolver.py:28\u001b[0m, in \u001b[0;36mConnect4MinimaxSolver.minimax\u001b[0;34m(self, depth, isMaximizingPlayer, alpha, beta, use_pruning)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mminimax\u001b[39m(\u001b[38;5;28mself\u001b[39m, depth, isMaximizingPlayer, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m'\u001b[39m), beta\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m'\u001b[39m), use_pruning\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# if datetime.now() > self.stop_time:\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m#     return 0, None\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgame\u001b[38;5;241m.\u001b[39mis_gameover():\n\u001b[1;32m     29\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscore \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     30\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchoice \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/MSc Computer Science/CS7IS2 - Artificial Intelligence/Assignments/Assignment 3/code/Connect4Game.py:59\u001b[0m, in \u001b[0;36mConnect4Game.is_gameover\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_gameover\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX_wins \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_winner(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX_wins:\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mO_wins \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_winner(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/MSc Computer Science/CS7IS2 - Artificial Intelligence/Assignments/Assignment 3/code/Connect4Game.py:42\u001b[0m, in \u001b[0;36mConnect4Game.is_winner\u001b[0;34m(self, player)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m7\u001b[39m):\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m3\u001b[39m):\n\u001b[0;32m---> 42\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mboard_status[row:row\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m4\u001b[39m, col] \u001b[38;5;241m==\u001b[39m player)\u001b[38;5;241m.\u001b[39mall():\n\u001b[1;32m     43\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Check for diagonal wins\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/anaconda3/lib/python3.11/site-packages/numpy/core/_methods.py:61\u001b[0m, in \u001b[0;36m_all\u001b[0;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m umr_any(a, axis, dtype, out, keepdims)\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m umr_any(a, axis, dtype, out, keepdims, where\u001b[38;5;241m=\u001b[39mwhere)\n\u001b[0;32m---> 61\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_all\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m, where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;66;03m# Parsing keyword arguments is currently fairly slow, so avoid it for now\u001b[39;00m\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m where \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m umr_all(a, axis, dtype, out, keepdims)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for depth in range(4,9):\n",
    "\n",
    "    # Create a game instance\n",
    "    game_instance = Connect4Game()\n",
    "\n",
    "    # Minimax agent plays against default opponent\n",
    "    agent = Connect4MinimaxSolver(game_instance, max_depth=depth, use_pruning=True)\n",
    "    opponent = Connect4DefaultSolver(game_instance)\n",
    "\n",
    "    scores_arr = []\n",
    "    for i in range(10):\n",
    "        start_time = time()\n",
    "        scores = evaluate_agent(game_instance, agent, opponent, num_games=100)\n",
    "        end_time = time()\n",
    "        scores['execution_time'] = end_time - start_time\n",
    "        scores_arr.append(scores)\n",
    "        print('Minimax vs Default')\n",
    "        print('X wins:', scores['X'])\n",
    "        print('O wins:', scores['O'])\n",
    "        print('Ties:', scores['tie'])\n",
    "\n",
    "    with open('experiments/c4-minimax-vs-default.txt', 'a') as f:\n",
    "        json.dump(scores_arr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total Progress: 100%|██████████| 625/625 [32:57<00:00,  3.16s/it]   \n"
     ]
    }
   ],
   "source": [
    "learning_rate_arr = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "discount_factor_arr = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "exploration_rate_arr = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "decay_rate_arr = [1e-6, 1e-5, 1e-4, 1e-3, 1e-2]\n",
    "scores_arr = []\n",
    "\n",
    "total_iterations = len(learning_rate_arr) * len(discount_factor_arr) * len(exploration_rate_arr) * len(decay_rate_arr)\n",
    "pbar = tqdm(total=total_iterations, desc='Total Progress')\n",
    "\n",
    "for learning_rate in learning_rate_arr:\n",
    "    for discount_factor in discount_factor_arr:\n",
    "        for exploration_rate in exploration_rate_arr:\n",
    "            for decay_rate in decay_rate_arr:\n",
    "                game_instance = TicTacToeGame()\n",
    "                agent = TicTacToeQLearningSolver(game_instance, learning_rate, discount_factor, exploration_rate, decay_rate)\n",
    "                opponent = TicTacToeRandomSolver(game_instance)\n",
    "                train_agent(agent, range(1000))\n",
    "                scores = evaluate_agent(game_instance, agent, opponent)\n",
    "                scores_arr.append((learning_rate, discount_factor, exploration_rate, decay_rate, scores['X'], scores['O'], scores['tie']))\n",
    "                pbar.update(1)\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>discount_factor</th>\n",
       "      <th>exploration_rate</th>\n",
       "      <th>decay_rate</th>\n",
       "      <th>X_wins</th>\n",
       "      <th>O_wins</th>\n",
       "      <th>ties</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>687</td>\n",
       "      <td>246</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>686</td>\n",
       "      <td>245</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>685</td>\n",
       "      <td>244</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>682</td>\n",
       "      <td>240</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>0.7</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>680</td>\n",
       "      <td>268</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     learning_rate  discount_factor  exploration_rate  decay_rate  X_wins  \\\n",
       "10             0.1              0.1               0.5    0.000001     687   \n",
       "81             0.1              0.7               0.3    0.000010     686   \n",
       "50             0.1              0.5               0.1    0.000001     685   \n",
       "300            0.5              0.5               0.1    0.000001     682   \n",
       "387            0.7              0.1               0.5    0.000100     680   \n",
       "\n",
       "     O_wins  ties  \n",
       "10      246    67  \n",
       "81      245    69  \n",
       "50      244    71  \n",
       "300     240    78  \n",
       "387     268    52  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Turn scores_arr into a dataframe\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(scores_arr, columns=['learning_rate', 'discount_factor', 'exploration_rate', 'decay_rate', 'X_wins', 'O_wins', 'ties'])\n",
    "\n",
    "# Sort by X_wins\n",
    "df = df.sort_values(by='X_wins', ascending=False)\n",
    "\n",
    "# Save to csv\n",
    "df.to_csv('q_learning_results.csv', index=False)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m agent \u001b[38;5;241m=\u001b[39m TicTacToeMinimaxSolver(game_instance)\n\u001b[1;32m      6\u001b[0m opponent \u001b[38;5;241m=\u001b[39m TicTacToeDefaultSolver(game_instance)\n\u001b[0;32m----> 7\u001b[0m scores \u001b[38;5;241m=\u001b[39m evaluate_agent(game_instance, agent, opponent)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMinimax vs Default\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX wins:\u001b[39m\u001b[38;5;124m'\u001b[39m, scores[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[0;32mIn[2], line 7\u001b[0m, in \u001b[0;36mevaluate_agent\u001b[0;34m(game_instance, agent, opponent)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m game_instance\u001b[38;5;241m.\u001b[39mis_gameover():\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m game_instance\u001b[38;5;241m.\u001b[39mplayer_X_turns:\n\u001b[0;32m----> 7\u001b[0m         agent\u001b[38;5;241m.\u001b[39mtake_turn()\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m      9\u001b[0m         opponent\u001b[38;5;241m.\u001b[39mtake_turn()\n",
      "File \u001b[0;32m~/Documents/MSc Computer Science/CS7IS2 - Artificial Intelligence/Assignments/Assignment 3/code/TicTacToeMinimaxSolver.py:61\u001b[0m, in \u001b[0;36mTicTacToeMinimaxSolver.take_turn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtake_turn\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 61\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mminimax(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgame\u001b[38;5;241m.\u001b[39mmake_move(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchoice)\n",
      "File \u001b[0;32m~/Documents/MSc Computer Science/CS7IS2 - Artificial Intelligence/Assignments/Assignment 3/code/TicTacToeMinimaxSolver.py:36\u001b[0m, in \u001b[0;36mTicTacToeMinimaxSolver.minimax\u001b[0;34m(self, depth, isMaximizingPlayer, alpha, beta, use_pruning)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     board[i][j] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 36\u001b[0m scores\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mminimax(depth, \u001b[38;5;129;01mnot\u001b[39;00m isMaximizingPlayer, alpha, beta, use_pruning))\n\u001b[1;32m     37\u001b[0m moves\u001b[38;5;241m.\u001b[39mappend([i, j])\n\u001b[1;32m     38\u001b[0m board[i][j] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/Documents/MSc Computer Science/CS7IS2 - Artificial Intelligence/Assignments/Assignment 3/code/TicTacToeMinimaxSolver.py:36\u001b[0m, in \u001b[0;36mTicTacToeMinimaxSolver.minimax\u001b[0;34m(self, depth, isMaximizingPlayer, alpha, beta, use_pruning)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     board[i][j] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 36\u001b[0m scores\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mminimax(depth, \u001b[38;5;129;01mnot\u001b[39;00m isMaximizingPlayer, alpha, beta, use_pruning))\n\u001b[1;32m     37\u001b[0m moves\u001b[38;5;241m.\u001b[39mappend([i, j])\n\u001b[1;32m     38\u001b[0m board[i][j] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "    \u001b[0;31m[... skipping similar frames: TicTacToeMinimaxSolver.minimax at line 36 (6 times)]\u001b[0m\n",
      "File \u001b[0;32m~/Documents/MSc Computer Science/CS7IS2 - Artificial Intelligence/Assignments/Assignment 3/code/TicTacToeMinimaxSolver.py:36\u001b[0m, in \u001b[0;36mTicTacToeMinimaxSolver.minimax\u001b[0;34m(self, depth, isMaximizingPlayer, alpha, beta, use_pruning)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     board[i][j] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 36\u001b[0m scores\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mminimax(depth, \u001b[38;5;129;01mnot\u001b[39;00m isMaximizingPlayer, alpha, beta, use_pruning))\n\u001b[1;32m     37\u001b[0m moves\u001b[38;5;241m.\u001b[39mappend([i, j])\n\u001b[1;32m     38\u001b[0m board[i][j] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/Documents/MSc Computer Science/CS7IS2 - Artificial Intelligence/Assignments/Assignment 3/code/TicTacToeMinimaxSolver.py:19\u001b[0m, in \u001b[0;36mTicTacToeMinimaxSolver.minimax\u001b[0;34m(self, depth, isMaximizingPlayer, alpha, beta, use_pruning)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mminimax\u001b[39m(\u001b[38;5;28mself\u001b[39m, depth, isMaximizingPlayer, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m'\u001b[39m), beta\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m'\u001b[39m), use_pruning\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m---> 19\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgame\u001b[38;5;241m.\u001b[39mis_gameover():\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscore \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     21\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchoice \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/MSc Computer Science/CS7IS2 - Artificial Intelligence/Assignments/Assignment 3/code/TicTacToeGame.py:65\u001b[0m, in \u001b[0;36mTicTacToeGame.is_gameover\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_gameover\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;66;03m# Either someone wins or all grid occupied\u001b[39;00m\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX_wins \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_winner(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX_wins:\n\u001b[1;32m     67\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mO_wins \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_winner(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/MSc Computer Science/CS7IS2 - Artificial Intelligence/Assignments/Assignment 3/code/TicTacToeGame.py:42\u001b[0m, in \u001b[0;36mTicTacToeGame.is_winner\u001b[0;34m(self, player)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mboard_status[i][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mboard_status[i][\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mboard_status[i][\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m==\u001b[39m player:\n\u001b[1;32m     41\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mboard_status[\u001b[38;5;241m0\u001b[39m][i] \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mboard_status[\u001b[38;5;241m1\u001b[39m][i] \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mboard_status[\u001b[38;5;241m2\u001b[39m][i] \u001b[38;5;241m==\u001b[39m player:\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Diagonals\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create a game instance\n",
    "game_instance = TicTacToeGame()\n",
    "\n",
    "# Minimax agent plays against default opponent\n",
    "agent = TicTacToeMinimaxSolver(game_instance)\n",
    "opponent = TicTacToeDefaultSolver(game_instance)\n",
    "scores = evaluate_agent(game_instance, agent, opponent)\n",
    "print('Minimax vs Default')\n",
    "print('X wins:', scores['X'])\n",
    "print('O wins:', scores['O'])\n",
    "print('Ties:', scores['tie'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TicTacToeQLearningSolver' object has no attribute 'make_move'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m untrained_scores \u001b[38;5;241m=\u001b[39m evaluate_agent(game_instance, q_learning_instance, random_opponent)\n\u001b[1;32m      2\u001b[0m scores_history \u001b[38;5;241m=\u001b[39m [untrained_scores]\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n",
      "Cell \u001b[0;32mIn[9], line 7\u001b[0m, in \u001b[0;36mevaluate_agent\u001b[0;34m(game_instance, agent, opponent)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m game_instance\u001b[38;5;241m.\u001b[39mis_gameover():\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m game_instance\u001b[38;5;241m.\u001b[39mplayer_X_turns:\n\u001b[0;32m----> 7\u001b[0m         agent\u001b[38;5;241m.\u001b[39mmake_move()\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m      9\u001b[0m         opponent\u001b[38;5;241m.\u001b[39mmake_move()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TicTacToeQLearningSolver' object has no attribute 'make_move'"
     ]
    }
   ],
   "source": [
    "untrained_scores = evaluate_agent(game_instance, q_learning_instance, random_opponent)\n",
    "scores_history = [untrained_scores]\n",
    "for i in range(10):\n",
    "    train_episodes = 10000\n",
    "    train_agent(q_learning_instance, episodes_range=range(i*train_episodes, (i+1)*train_episodes))\n",
    "    scores = evaluate_agent(game_instance, q_learning_instance, random_opponent)\n",
    "    scores_history.append(scores)\n",
    "    print('Q-learning vs Random - Game {}: X wins: {}, O wins: {}, tie: {}'.format(i, scores['X'], scores['O'], scores['tie']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'X': 1000, 'O': 0, 'tie': 0}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_history"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
